{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data_2/drug4k_smiles-embeddings.xlsx ...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train a Variational Autoencoder (VAE) on DRUG-seq compound embeddings.\n",
    "\n",
    "Input:\n",
    "    data_2/drug4k_smiles-embeddings.xlsx\n",
    "\n",
    "Columns expected:\n",
    "    cmpd_sample_id, inchi_key, smiles, cas_number, moa, 0, 1, 2, ...\n",
    "\n",
    "Output:\n",
    "    data_2/vae_drug4k.pt                       - VAE state dict\n",
    "    data_2/vae_drug4k_latent.parquet          - latent vectors per compound\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "\n",
    "DATA_PATH = Path(\"data_2\") / \"drug4k_smiles-embeddings.xlsx\"\n",
    "OUT_DIR = Path(\"data_2\")\n",
    "\n",
    "LATENT_DIM = 32          # size of VAE latent space\n",
    "HIDDEN_DIM = 128         # hidden layer size\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "N_EPOCHS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data loading and preprocessing\n",
    "# -----------------------------\n",
    "\n",
    "print(f\"Loading data from {DATA_PATH} ...\")\n",
    "# Suppress openpyxl warnings about invalid date cells\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# Keep id columns for later, embeddings for training\n",
    "id_cols = [\"cmpd_sample_id\", \"inchi_key\", \"smiles\", \"cas_number\", \"moa\"]\n",
    "id_cols = [c for c in id_cols if c in df.columns]\n",
    "\n",
    "# Numeric embedding columns (0, 1, 2, ...)\n",
    "emb_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Found {len(emb_cols)} embedding dimensions\")\n",
    "\n",
    "X = df[emb_cols].to_numpy().astype(np.float32)\n",
    "\n",
    "# Normalize embeddings\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X).astype(np.float32)\n",
    "\n",
    "print(f\"Data shape: {X_scaled.shape}\")\n",
    "\n",
    "# Split data into train/validation/test sets\n",
    "# First split: 85% train+val, 15% test\n",
    "X_train_val, X_test, idx_train_val, idx_test = train_test_split(\n",
    "    X_scaled, np.arange(len(X_scaled)), test_size=0.15, random_state=42\n",
    ")\n",
    "# Second split: 70% train, 15% validation (of total)\n",
    "X_train, X_val, idx_train, idx_val = train_test_split(\n",
    "    X_train_val, idx_train_val, test_size=0.1765, random_state=42  # 0.1765 ≈ 15/85\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples ({len(X_train)/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X_scaled)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.from_numpy(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "\n",
    "# Create datasets - only train data for training\n",
    "train_dataset = EmbDataset(X_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "# Also create validation loader for evaluation during training\n",
    "val_dataset = EmbDataset(X_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# VAE model\n",
    "# -----------------------------\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder - shared layers\n",
    "        self.fc_dec1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        # Separate output layers for mean and log-variance of gene expression distribution\n",
    "        self.fc_out_mean = nn.Linear(hidden_dim, input_dim)\n",
    "        self.fc_out_logvar = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Shared hidden layer\n",
    "        h = self.relu(self.fc_dec1(z))\n",
    "        # Separate outputs for mean and log-variance\n",
    "        recon_mean = self.fc_out_mean(h)\n",
    "        recon_logvar = self.fc_out_logvar(h)\n",
    "        return recon_mean, recon_logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_mean, recon_logvar = self.decode(z)\n",
    "        return recon_mean, recon_logvar, mu, logvar\n",
    "\n",
    "\n",
    "input_dim = X_scaled.shape[1]\n",
    "model = VAE(input_dim=input_dim, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "def vae_loss(recon_mean, recon_logvar, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    VAE loss with negative log likelihood for gene expression reconstruction.\n",
    "    \n",
    "    Args:\n",
    "        recon_mean: Predicted mean of gene expression distribution [batch, features]\n",
    "        recon_logvar: Predicted log-variance of gene expression distribution [batch, features]\n",
    "        x: True gene expression values [batch, features]\n",
    "        mu: Encoder mean [batch, latent_dim]\n",
    "        logvar: Encoder log-variance [batch, latent_dim]\n",
    "    \"\"\"\n",
    "    # Negative log likelihood for normal distribution\n",
    "    # NLL = -log p(x | μ, σ²) = 0.5 * log(2π) + 0.5 * log(σ²) + 0.5 * (x - μ)²/σ²\n",
    "    # where logvar = log(σ²)\n",
    "    # NLL = 0.5 * log(2π) + 0.5 * logvar + 0.5 * (x - μ)² * exp(-logvar)\n",
    "    const = 0.5 * np.log(2.0 * np.pi)\n",
    "    recon_loss = torch.sum(\n",
    "        const + 0.5 * recon_logvar + 0.5 * (x - recon_mean).pow(2) * torch.exp(-recon_logvar)\n",
    "    )\n",
    "    \n",
    "    # KL divergence\n",
    "    kld = 0.5 * torch.sum(torch.exp(logvar) + mu.pow(2) - 1.0 - logvar)\n",
    "    \n",
    "    return recon_loss + kld, recon_loss, kld\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training loop\n",
    "# -----------------------------\n",
    "\n",
    "# Track training metrics\n",
    "train_losses = []\n",
    "train_recon_losses = []\n",
    "train_kld_losses = []\n",
    "val_losses = []\n",
    "val_recon_losses = []\n",
    "val_kld_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kld = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_mean, recon_logvar, mu, logvar = model(batch)\n",
    "        loss, recon_loss, kld = vae_loss(recon_mean, recon_logvar, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = batch.size(0)\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon_loss.item()\n",
    "        total_kld += kld.item()\n",
    "        n_samples += bs\n",
    "\n",
    "    avg_loss = total_loss / n_samples\n",
    "    avg_recon = total_recon / n_samples\n",
    "    avg_kld = total_kld / n_samples\n",
    "\n",
    "    # Track training metrics\n",
    "    train_losses.append(avg_loss)\n",
    "    train_recon_losses.append(avg_recon)\n",
    "    train_kld_losses.append(avg_kld)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_total_recon = 0.0\n",
    "    val_total_kld = 0.0\n",
    "    val_n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            recon_mean, recon_logvar, mu, logvar = model(batch)\n",
    "            loss, recon_loss, kld = vae_loss(recon_mean, recon_logvar, batch, mu, logvar)\n",
    "\n",
    "            bs = batch.size(0)\n",
    "            val_total_loss += loss.item()\n",
    "            val_total_recon += recon_loss.item()\n",
    "            val_total_kld += kld.item()\n",
    "            val_n_samples += bs\n",
    "\n",
    "    val_avg_loss = val_total_loss / val_n_samples\n",
    "    val_avg_recon = val_total_recon / val_n_samples\n",
    "    val_avg_kld = val_total_kld / val_n_samples\n",
    "\n",
    "    # Track validation metrics\n",
    "    val_losses.append(val_avg_loss)\n",
    "    val_recon_losses.append(val_avg_recon)\n",
    "    val_kld_losses.append(val_avg_kld)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"Train: loss {avg_loss:.4f} recon {avg_recon:.4f} kld {avg_kld:.4f} | \"\n",
    "        f\"Val: loss {val_avg_loss:.4f} recon {val_avg_recon:.4f} kld {val_avg_kld:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Plot training curves\n",
    "# -----------------------------\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "epochs = range(1, N_EPOCHS + 1)\n",
    "\n",
    "# Plot 1: Total loss (train vs validation)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss', alpha=0.8)\n",
    "plt.plot(epochs, val_losses, 'b--', linewidth=2, label='Val Loss', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Reconstruction and KL divergence losses (train vs validation)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, train_recon_losses, 'r-', linewidth=2, label='Train Recon', alpha=0.8)\n",
    "plt.plot(epochs, val_recon_losses, 'r--', linewidth=2, label='Val Recon', alpha=0.8)\n",
    "plt.plot(epochs, train_kld_losses, 'g-', linewidth=2, label='Train KL', alpha=0.8)\n",
    "plt.plot(epochs, val_kld_losses, 'g--', linewidth=2, label='Val KL', alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Components')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=8)\n",
    "\n",
    "# Plot 3: Log scale for better visualization\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss', alpha=0.8)\n",
    "plt.plot(epochs, val_losses, 'b--', linewidth=2, label='Val Loss', alpha=0.8)\n",
    "plt.plot(epochs, train_recon_losses, 'r-', linewidth=1.5, label='Train Recon', alpha=0.7)\n",
    "plt.plot(epochs, val_recon_losses, 'r--', linewidth=1.5, label='Val Recon', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('All Losses (Log Scale)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"vae_training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nSaved training curves to {OUT_DIR / 'vae_training_curves.png'}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation metrics on Train/Val/Test sets\n",
    "# -----------------------------\n",
    "\n",
    "def compute_metrics(X_true, X_recon, set_name):\n",
    "    \"\"\"Compute reconstruction metrics for a dataset.\"\"\"\n",
    "    # Flatten for metric computation\n",
    "    X_true_flat = X_true.flatten()\n",
    "    X_recon_flat = X_recon.flatten()\n",
    "    \n",
    "    # Compute metrics\n",
    "    r2 = r2_score(X_true_flat, X_recon_flat)\n",
    "    mse = mean_squared_error(X_true_flat, X_recon_flat)\n",
    "    mae = mean_absolute_error(X_true_flat, X_recon_flat)\n",
    "    explained_var = explained_variance_score(X_true_flat, X_recon_flat)\n",
    "    \n",
    "    # Per-sample R²\n",
    "    sample_r2s = []\n",
    "    for i in range(len(X_true)):\n",
    "        sample_r2s.append(r2_score(X_true[i], X_recon[i]))\n",
    "    sample_r2s = np.array(sample_r2s)\n",
    "    \n",
    "    return {\n",
    "        'r2': r2,\n",
    "        'explained_var': explained_var,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'sample_r2_mean': sample_r2s.mean(),\n",
    "        'sample_r2_median': np.median(sample_r2s),\n",
    "        'sample_r2_std': sample_r2s.std(),\n",
    "        'sample_r2_min': sample_r2s.min(),\n",
    "        'sample_r2_max': sample_r2s.max(),\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Computing evaluation metrics on Train/Val/Test sets...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "metrics = {}\n",
    "\n",
    "# Evaluate on training set\n",
    "with torch.no_grad():\n",
    "    X_train_tensor = torch.from_numpy(X_train).to(DEVICE)\n",
    "    X_train_recon_mean, X_train_recon_logvar, _, _ = model(X_train_tensor)\n",
    "    X_train_recon = X_train_recon_mean.cpu().numpy()\n",
    "    X_train_recon_logvar_np = X_train_recon_logvar.cpu().numpy()\n",
    "metrics['train'] = compute_metrics(X_train, X_train_recon, 'Train')\n",
    "\n",
    "# Evaluate on validation set\n",
    "with torch.no_grad():\n",
    "    X_val_tensor = torch.from_numpy(X_val).to(DEVICE)\n",
    "    X_val_recon_mean, X_val_recon_logvar, _, _ = model(X_val_tensor)\n",
    "    X_val_recon = X_val_recon_mean.cpu().numpy()\n",
    "    X_val_recon_logvar_np = X_val_recon_logvar.cpu().numpy()\n",
    "metrics['val'] = compute_metrics(X_val, X_val_recon, 'Val')\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.from_numpy(X_test).to(DEVICE)\n",
    "    X_test_recon_mean, X_test_recon_logvar, _, _ = model(X_test_tensor)\n",
    "    X_test_recon = X_test_recon_mean.cpu().numpy()\n",
    "    X_test_recon_logvar_np = X_test_recon_logvar.cpu().numpy()\n",
    "metrics['test'] = compute_metrics(X_test, X_test_recon, 'Test')\n",
    "\n",
    "# Print metrics\n",
    "for set_name in ['train', 'val', 'test']:\n",
    "    m = metrics[set_name]\n",
    "    print(f\"\\n{set_name.upper()} Set Metrics:\")\n",
    "    print(f\"  R² Score:           {m['r2']:.6f}\")\n",
    "    print(f\"  Explained Variance: {m['explained_var']:.6f}\")\n",
    "    print(f\"  Mean Squared Error: {m['mse']:.6f}\")\n",
    "    print(f\"  Mean Absolute Error: {m['mae']:.6f}\")\n",
    "    print(f\"  RMSE:               {m['rmse']:.6f}\")\n",
    "    print(f\"  Per-sample R² - Mean: {m['sample_r2_mean']:.6f}, Median: {m['sample_r2_median']:.6f}, Std: {m['sample_r2_std']:.6f}\")\n",
    "\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute 95% Confidence Interval Coverage\n",
    "# -----------------------------\n",
    "\n",
    "def compute_ci_coverage(X_true, X_pred_mean, X_pred_logvar, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Compute the percentage of true values within predicted confidence intervals.\n",
    "    \n",
    "    Args:\n",
    "        X_true: True values [n_samples, n_features]\n",
    "        X_pred_mean: Predicted means [n_samples, n_features]\n",
    "        X_pred_logvar: Predicted log-variances [n_samples, n_features]\n",
    "        confidence: Confidence level (default 0.95 for 95% CI)\n",
    "    \n",
    "    Returns:\n",
    "        Coverage percentage (0-100)\n",
    "    \"\"\"\n",
    "    # Convert log-variance to standard deviation\n",
    "    std = np.exp(0.5 * X_pred_logvar)\n",
    "    \n",
    "    # Compute z-score for confidence interval (1.96 for 95%, 2.576 for 99%)\n",
    "    z_score = 1.96 if confidence == 0.95 else 2.576 if confidence == 0.99 else None\n",
    "    if z_score is None:\n",
    "        raise ValueError(f\"Confidence level {confidence} not supported. Use 0.95 or 0.99\")\n",
    "    \n",
    "    # Compute confidence intervals\n",
    "    lower_bound = X_pred_mean - z_score * std\n",
    "    upper_bound = X_pred_mean + z_score * std\n",
    "    \n",
    "    # Check if true values are within intervals\n",
    "    within_ci = (X_true >= lower_bound) & (X_true <= upper_bound)\n",
    "    \n",
    "    # Compute coverage percentage\n",
    "    coverage = 100.0 * within_ci.sum() / within_ci.size\n",
    "    \n",
    "    return coverage\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"95% Confidence Interval Coverage:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute coverage for each set\n",
    "train_ci_coverage = compute_ci_coverage(X_train, X_train_recon, X_train_recon_logvar_np, confidence=0.95)\n",
    "val_ci_coverage = compute_ci_coverage(X_val, X_val_recon, X_val_recon_logvar_np, confidence=0.95)\n",
    "test_ci_coverage = compute_ci_coverage(X_test, X_test_recon, X_test_recon_logvar_np, confidence=0.95)\n",
    "\n",
    "print(f\"\\nTrain Set:  {train_ci_coverage:.2f}% of values within predicted 95% CI\")\n",
    "print(f\"Val Set:    {val_ci_coverage:.2f}% of values within predicted 95% CI\")\n",
    "print(f\"Test Set:   {test_ci_coverage:.2f}% of values within predicted 95% CI\")\n",
    "print(f\"\\nExpected:   ~95% (if predictions are well-calibrated)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Add CI coverage to metrics\n",
    "metrics['train']['ci_coverage_95'] = train_ci_coverage\n",
    "metrics['val']['ci_coverage_95'] = val_ci_coverage\n",
    "metrics['test']['ci_coverage_95'] = test_ci_coverage\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Save model and latent space\n",
    "# -----------------------------\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model_path = OUT_DIR / \"vae_drug4k.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"input_dim\": input_dim,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"latent_dim\": LATENT_DIM,\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"emb_cols\": emb_cols,\n",
    "        \"metrics\": {\n",
    "            \"train\": {\n",
    "                \"r2_score\": float(metrics['train']['r2']),\n",
    "                \"explained_variance\": float(metrics['train']['explained_var']),\n",
    "                \"mse\": float(metrics['train']['mse']),\n",
    "                \"mae\": float(metrics['train']['mae']),\n",
    "                \"rmse\": float(metrics['train']['rmse']),\n",
    "                \"per_sample_r2\": {\n",
    "                    \"mean\": float(metrics['train']['sample_r2_mean']),\n",
    "                    \"median\": float(metrics['train']['sample_r2_median']),\n",
    "                    \"std\": float(metrics['train']['sample_r2_std']),\n",
    "                    \"min\": float(metrics['train']['sample_r2_min']),\n",
    "                    \"max\": float(metrics['train']['sample_r2_max']),\n",
    "                }\n",
    "            },\n",
    "            \"validation\": {\n",
    "                \"r2_score\": float(metrics['val']['r2']),\n",
    "                \"explained_variance\": float(metrics['val']['explained_var']),\n",
    "                \"mse\": float(metrics['val']['mse']),\n",
    "                \"mae\": float(metrics['val']['mae']),\n",
    "                \"rmse\": float(metrics['val']['rmse']),\n",
    "                \"per_sample_r2\": {\n",
    "                    \"mean\": float(metrics['val']['sample_r2_mean']),\n",
    "                    \"median\": float(metrics['val']['sample_r2_median']),\n",
    "                    \"std\": float(metrics['val']['sample_r2_std']),\n",
    "                    \"min\": float(metrics['val']['sample_r2_min']),\n",
    "                    \"max\": float(metrics['val']['sample_r2_max']),\n",
    "                }\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"r2_score\": float(metrics['test']['r2']),\n",
    "                \"explained_variance\": float(metrics['test']['explained_var']),\n",
    "                \"mse\": float(metrics['test']['mse']),\n",
    "                \"mae\": float(metrics['test']['mae']),\n",
    "                \"rmse\": float(metrics['test']['rmse']),\n",
    "                \"per_sample_r2\": {\n",
    "                    \"mean\": float(metrics['test']['sample_r2_mean']),\n",
    "                    \"median\": float(metrics['test']['sample_r2_median']),\n",
    "                    \"std\": float(metrics['test']['sample_r2_std']),\n",
    "                    \"min\": float(metrics['test']['sample_r2_min']),\n",
    "                    \"max\": float(metrics['test']['sample_r2_max']),\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    model_path,\n",
    ")\n",
    "print(f\"Saved VAE model to {model_path}\")\n",
    "\n",
    "# Compute latent vectors for all compounds (train + val + test)\n",
    "# Combine all data in original order\n",
    "all_indices = np.concatenate([idx_train, idx_val, idx_test])\n",
    "all_data = np.concatenate([X_train, X_val, X_test])\n",
    "\n",
    "# Reorder to match original dataframe order\n",
    "sort_order = np.argsort(all_indices)\n",
    "all_data_ordered = all_data[sort_order]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_all_tensor = torch.from_numpy(all_data_ordered).to(DEVICE)\n",
    "    mu, logvar = model.encode(X_all_tensor)\n",
    "    z = mu.cpu().numpy()  # use mean as latent embedding\n",
    "\n",
    "latent_cols = [f\"z_{i}\" for i in range(LATENT_DIM)]\n",
    "latent_df = pd.DataFrame(z, columns=latent_cols)\n",
    "\n",
    "# Attach ids if present\n",
    "for c in id_cols:\n",
    "    latent_df[c] = df[c].values\n",
    "\n",
    "# Convert object columns to strings to handle datetime objects and mixed types\n",
    "for c in id_cols:\n",
    "    if latent_df[c].dtype == 'object':\n",
    "        latent_df[c] = latent_df[c].astype(str)\n",
    "\n",
    "latent_out_path = OUT_DIR / \"vae_drug4k_latent.parquet\"\n",
    "latent_df.to_parquet(latent_out_path, index=False)\n",
    "print(f\"Saved latent vectors to {latent_out_path}\")\n",
    "print(latent_df.head())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MLP Autoencoder Architecture (Non-Variational)\n",
    "# ============================================================================\n",
    "\n",
    "class MLPAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP-based autoencoder without variational constraints.\n",
    "    Predicts distribution parameters (mean and log-variance) similar to VAE.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder_fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.encoder_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.encoder_out = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder - shared layers\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Separate output layers for mean and log-variance\n",
    "        self.decoder_out_mean = nn.Linear(hidden_dim, input_dim)\n",
    "        self.decoder_out_logvar = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent representation.\"\"\"\n",
    "        h = self.relu(self.encoder_fc1(x))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.encoder_fc2(h))\n",
    "        z = self.encoder_out(h)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent representation to distribution parameters.\"\"\"\n",
    "        h = self.relu(self.decoder_fc1(z))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.decoder_fc2(h))\n",
    "        recon_mean = self.decoder_out_mean(h)\n",
    "        recon_logvar = self.decoder_out_logvar(h)\n",
    "        return recon_mean, recon_logvar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        z = self.encode(x)\n",
    "        recon_mean, recon_logvar = self.decode(z)\n",
    "        return recon_mean, recon_logvar\n",
    "\n",
    "\n",
    "def mlp_loss(recon_mean, recon_logvar, x):\n",
    "    \"\"\"\n",
    "    Loss function for MLP autoencoder (NLL only, no KL term).\n",
    "    \"\"\"\n",
    "    # Negative log likelihood for normal distribution\n",
    "    const = 0.5 * np.log(2.0 * np.pi)\n",
    "    recon_loss = torch.sum(\n",
    "        const + 0.5 * recon_logvar + 0.5 * (x - recon_mean).pow(2) * torch.exp(-recon_logvar)\n",
    "    )\n",
    "    return recon_loss, recon_loss\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Benchmark: Train MLP Autoencoder and Compare with VAE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARKING: MLP Autoencoder vs VAE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize MLP model\n",
    "mlp_model = MLPAutoencoder(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    latent_dim=LATENT_DIM\n",
    ").to(DEVICE)\n",
    "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"\\nMLP Model Architecture:\")\n",
    "print(mlp_model)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in mlp_model.parameters()):,}\")\n",
    "print(f\"VAE Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Training metrics for MLP\n",
    "mlp_train_losses = []\n",
    "mlp_val_losses = []\n",
    "\n",
    "print(f\"\\nTraining MLP Autoencoder for {N_EPOCHS} epochs...\")\n",
    "mlp_model.train()\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    # Training phase\n",
    "    mlp_model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        mlp_optimizer.zero_grad()\n",
    "        recon_mean, recon_logvar = mlp_model(batch)\n",
    "        loss, _ = mlp_loss(recon_mean, recon_logvar, batch)\n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "        \n",
    "        bs = batch.size(0)\n",
    "        total_loss += loss.item()\n",
    "        n_samples += bs\n",
    "    \n",
    "    avg_loss = total_loss / n_samples\n",
    "    mlp_train_losses.append(avg_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    mlp_model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    val_n_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            recon_mean, recon_logvar = mlp_model(batch)\n",
    "            loss, _ = mlp_loss(recon_mean, recon_logvar, batch)\n",
    "            \n",
    "            bs = batch.size(0)\n",
    "            val_total_loss += loss.item()\n",
    "            val_n_samples += bs\n",
    "    \n",
    "    val_avg_loss = val_total_loss / val_n_samples\n",
    "    mlp_val_losses.append(val_avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {avg_loss:.4f} | Val Loss: {val_avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING MLP MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate MLP on all sets\n",
    "mlp_metrics = {}\n",
    "\n",
    "# Evaluate on training set\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_train_tensor = torch.from_numpy(X_train).to(DEVICE)\n",
    "    X_train_mlp_mean, X_train_mlp_logvar = mlp_model(X_train_tensor)\n",
    "    X_train_mlp_recon = X_train_mlp_mean.cpu().numpy()\n",
    "    X_train_mlp_logvar_np = X_train_mlp_logvar.cpu().numpy()\n",
    "mlp_metrics['train'] = compute_metrics(X_train, X_train_mlp_recon, 'Train')\n",
    "\n",
    "# Evaluate on validation set\n",
    "with torch.no_grad():\n",
    "    X_val_tensor = torch.from_numpy(X_val).to(DEVICE)\n",
    "    X_val_mlp_mean, X_val_mlp_logvar = mlp_model(X_val_tensor)\n",
    "    X_val_mlp_recon = X_val_mlp_mean.cpu().numpy()\n",
    "    X_val_mlp_logvar_np = X_val_mlp_logvar.cpu().numpy()\n",
    "mlp_metrics['val'] = compute_metrics(X_val, X_val_mlp_recon, 'Val')\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.from_numpy(X_test).to(DEVICE)\n",
    "    X_test_mlp_mean, X_test_mlp_logvar = mlp_model(X_test_tensor)\n",
    "    X_test_mlp_recon = X_test_mlp_mean.cpu().numpy()\n",
    "    X_test_mlp_logvar_np = X_test_mlp_logvar.cpu().numpy()\n",
    "mlp_metrics['test'] = compute_metrics(X_test, X_test_mlp_recon, 'Test')\n",
    "\n",
    "# Compute CI coverage for MLP\n",
    "mlp_train_ci = compute_ci_coverage(X_train, X_train_mlp_recon, X_train_mlp_logvar_np, confidence=0.95)\n",
    "mlp_val_ci = compute_ci_coverage(X_val, X_val_mlp_recon, X_val_mlp_logvar_np, confidence=0.95)\n",
    "mlp_test_ci = compute_ci_coverage(X_test, X_test_mlp_recon, X_test_mlp_logvar_np, confidence=0.95)\n",
    "\n",
    "mlp_metrics['train']['ci_coverage_95'] = mlp_train_ci\n",
    "mlp_metrics['val']['ci_coverage_95'] = mlp_val_ci\n",
    "mlp_metrics['test']['ci_coverage_95'] = mlp_test_ci\n",
    "\n",
    "# ============================================================================\n",
    "# Comparison Table\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: VAE vs MLP Autoencoder\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate and display parameter counts\n",
    "vae_params = sum(p.numel() for p in model.parameters())\n",
    "mlp_params = sum(p.numel() for p in mlp_model.parameters())\n",
    "vae_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "mlp_trainable = sum(p.numel() for p in mlp_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nArchitecture Summary:\")\n",
    "print(f\"{'Metric':<25} {'VAE':<20} {'MLP':<20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Total Parameters':<25} {vae_params:>19,} {mlp_params:>19,}\")\n",
    "print(f\"{'Trainable Parameters':<25} {vae_trainable:>19,} {mlp_trainable:>19,}\")\n",
    "print(f\"{'Parameter Difference':<25} {'-'*19} {mlp_params - vae_params:>+19,} ({((mlp_params - vae_params) / vae_params * 100):>+6.1f}%)\")\n",
    "print(f\"{'Architecture Type':<25} {'Variational':<20} {'Standard':<20}\")\n",
    "print(f\"{'KL Regularization':<25} {'Yes (adds constraint)':<20} {'No':<20}\")\n",
    "\n",
    "comparison_metrics = ['r2', 'mse', 'mae', 'rmse', 'ci_coverage_95']\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"{'Metric':<20} {'VAE':<15} {'MLP':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for metric in comparison_metrics:\n",
    "    vae_val = metrics['test'][metric]\n",
    "    mlp_val = mlp_metrics['test'][metric]\n",
    "    diff = mlp_val - vae_val\n",
    "    diff_pct = (diff / vae_val * 100) if vae_val != 0 else 0\n",
    "    \n",
    "    metric_name = metric.replace('_', ' ').title()\n",
    "    if metric == 'ci_coverage_95':\n",
    "        print(f\"{metric_name:<20} {vae_val:>6.2f}%      {mlp_val:>6.2f}%      {diff:>+6.2f}% ({diff_pct:>+5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{metric_name:<20} {vae_val:>14.6f} {mlp_val:>14.6f} {diff:>+14.6f} ({diff_pct:>+5.1f}%)\")\n",
    "\n",
    "print(\"\\nValidation Set Metrics:\")\n",
    "print(f\"{'Metric':<20} {'VAE':<15} {'MLP':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for metric in comparison_metrics:\n",
    "    vae_val = metrics['val'][metric]\n",
    "    mlp_val = mlp_metrics['val'][metric]\n",
    "    diff = mlp_val - vae_val\n",
    "    diff_pct = (diff / vae_val * 100) if vae_val != 0 else 0\n",
    "    \n",
    "    metric_name = metric.replace('_', ' ').title()\n",
    "    if metric == 'ci_coverage_95':\n",
    "        print(f\"{metric_name:<20} {vae_val:>6.2f}%      {mlp_val:>6.2f}%      {diff:>+6.2f}% ({diff_pct:>+5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"{metric_name:<20} {vae_val:>14.6f} {mlp_val:>14.6f} {diff:>+14.6f} ({diff_pct:>+5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Comparison Plot\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "epochs = range(1, N_EPOCHS + 1)\n",
    "\n",
    "# Plot 1: Training loss comparison\n",
    "axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2, label='VAE Train', alpha=0.8)\n",
    "axes[0, 0].plot(epochs, val_losses, 'b--', linewidth=2, label='VAE Val', alpha=0.8)\n",
    "axes[0, 0].plot(epochs, mlp_train_losses, 'r-', linewidth=2, label='MLP Train', alpha=0.8)\n",
    "axes[0, 0].plot(epochs, mlp_val_losses, 'r--', linewidth=2, label='MLP Val', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test set R² comparison\n",
    "sets = ['Train', 'Val', 'Test']\n",
    "vae_r2 = [metrics['train']['r2'], metrics['val']['r2'], metrics['test']['r2']]\n",
    "mlp_r2 = [mlp_metrics['train']['r2'], mlp_metrics['val']['r2'], mlp_metrics['test']['r2']]\n",
    "x = np.arange(len(sets))\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, vae_r2, width, label='VAE', alpha=0.8)\n",
    "axes[0, 1].bar(x + width/2, mlp_r2, width, label='MLP', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Dataset')\n",
    "axes[0, 1].set_ylabel('R² Score')\n",
    "axes[0, 1].set_title('R² Score Comparison')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(sets)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: CI Coverage comparison\n",
    "vae_ci = [metrics['train']['ci_coverage_95'], metrics['val']['ci_coverage_95'], metrics['test']['ci_coverage_95']]\n",
    "mlp_ci = [mlp_metrics['train']['ci_coverage_95'], mlp_metrics['val']['ci_coverage_95'], mlp_metrics['test']['ci_coverage_95']]\n",
    "axes[1, 0].bar(x - width/2, vae_ci, width, label='VAE', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, mlp_ci, width, label='MLP', alpha=0.8)\n",
    "axes[1, 0].axhline(y=95, color='g', linestyle='--', label='Expected 95%', alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Dataset')\n",
    "axes[1, 0].set_ylabel('CI Coverage (%)')\n",
    "axes[1, 0].set_title('95% CI Coverage Comparison')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(sets)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: RMSE comparison\n",
    "vae_rmse = [metrics['train']['rmse'], metrics['val']['rmse'], metrics['test']['rmse']]\n",
    "mlp_rmse = [mlp_metrics['train']['rmse'], mlp_metrics['val']['rmse'], mlp_metrics['test']['rmse']]\n",
    "axes[1, 1].bar(x - width/2, vae_rmse, width, label='VAE', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, mlp_rmse, width, label='MLP', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Dataset')\n",
    "axes[1, 1].set_ylabel('RMSE')\n",
    "axes[1, 1].set_title('RMSE Comparison')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(sets)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "benchmark_plot_path = OUT_DIR / \"vae_vs_mlp_benchmark.png\"\n",
    "plt.savefig(benchmark_plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nSaved benchmark comparison plot to {benchmark_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Save MLP model\n",
    "mlp_model_path = OUT_DIR / \"mlp_drug4k.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"state_dict\": mlp_model.state_dict(),\n",
    "        \"input_dim\": input_dim,\n",
    "        \"hidden_dim\": HIDDEN_DIM,\n",
    "        \"latent_dim\": LATENT_DIM,\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"emb_cols\": emb_cols,\n",
    "        \"metrics\": {\n",
    "            \"train\": {k: float(v) for k, v in mlp_metrics['train'].items()},\n",
    "            \"validation\": {k: float(v) for k, v in mlp_metrics['val'].items()},\n",
    "            \"test\": {k: float(v) for k, v in mlp_metrics['test'].items()}\n",
    "        }\n",
    "    },\n",
    "    mlp_model_path,\n",
    ")\n",
    "print(f\"Saved MLP model to {mlp_model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
